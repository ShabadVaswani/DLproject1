{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQiXJvEv9_Q2",
        "outputId": "b65086b2-fff9-4a67-ed53-fbdfec3b54ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOI9HLz19_1r"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5fR1SAF_u-r"
      },
      "outputs": [],
      "source": [
        "# Template code for reading the test file\n",
        "def load_cifar_batch(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        batch = pickle.load(fo, encoding='bytes')\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJdbXLVy_wRu"
      },
      "outputs": [],
      "source": [
        "# Get Train Test Loader\n",
        "def get_data(batch_size=128):\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomAdjustSharpness(sharpness_factor = 2,p = 0.2),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness = 0.1,contrast = 0.1,saturation = 0.1),\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "            transforms.RandomErasing(p=0.2,scale=(0.02, 0.1),value=1.0, inplace=False),\n",
        "        ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "        ])\n",
        "\n",
        "        dataset_path = \"/content/drive/MyDrive/dl project1/data\"\n",
        "        train_dataset = torchvision.datasets.CIFAR10(root=dataset_path, train=True,\n",
        "                                                    download=False, transform=transform_train)\n",
        "        test_dataset = torchvision.datasets.CIFAR10(root=dataset_path, train=False,\n",
        "                                                    download=False, transform=transform_test)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "        return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEhkVxEa_xsd"
      },
      "outputs": [],
      "source": [
        "# Define Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        return F.relu(out)\n",
        "\n",
        "# Define Optimized ResNet Model (Under 5M Params)\n",
        "class OptimizedResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(OptimizedResNet, self).__init__()\n",
        "        # Reduce Initial Convolution Filters\n",
        "        # Increase Initial Convolution Filters\n",
        "        self.conv1 = nn.Conv2d(3, 52, kernel_size=3, stride=1, padding=1, bias=False)  # 54 → 52\n",
        "        self.bn1 = nn.BatchNorm2d(52)\n",
        "\n",
        "        # Adjust Layer Channels and Depth\n",
        "        self.layer1 = self._make_layer(52, 96, 3, stride=1)\n",
        "        self.layer2 = self._make_layer(96, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 240, 2, stride=2)  # 248 → 240\n",
        "        self.layer4 = self._make_layer(240, 344, 1, stride=2)  # 352 → 344\n",
        "\n",
        "        # Adaptive Pooling to Ensure Fixed Output Size\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Adjust Fully Connected Layer\n",
        "        self.fc = nn.Linear(344, num_classes)  # 352 → 344\n",
        "\n",
        "\n",
        "\n",
        "        # Apply weight initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "        # Print model summary and parameter count\n",
        "        self.print_model_info()\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
        "        layers = [ResidualBlock(in_channels, out_channels, stride)]\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def print_model_info(self):\n",
        "        \"\"\"Prints model summary and total trainable parameters\"\"\"\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.to(device)\n",
        "        summary(self, (3, 32, 32))\n",
        "        num_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "        print(f\"\\nTotal Trainable Parameters: {num_params:,} ({num_params/1e6:.2f}M)\\n\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5F9HMsu_0Dk",
        "outputId": "cadb775c-efcc-4b9a-be9f-923072ec6cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 52, 32, 32]           1,404\n",
            "       BatchNorm2d-2           [-1, 52, 32, 32]             104\n",
            "            Conv2d-3           [-1, 96, 32, 32]           4,992\n",
            "       BatchNorm2d-4           [-1, 96, 32, 32]             192\n",
            "            Conv2d-5           [-1, 96, 32, 32]          44,928\n",
            "       BatchNorm2d-6           [-1, 96, 32, 32]             192\n",
            "            Conv2d-7           [-1, 96, 32, 32]          82,944\n",
            "       BatchNorm2d-8           [-1, 96, 32, 32]             192\n",
            "     ResidualBlock-9           [-1, 96, 32, 32]               0\n",
            "           Conv2d-10           [-1, 96, 32, 32]          82,944\n",
            "      BatchNorm2d-11           [-1, 96, 32, 32]             192\n",
            "           Conv2d-12           [-1, 96, 32, 32]          82,944\n",
            "      BatchNorm2d-13           [-1, 96, 32, 32]             192\n",
            "    ResidualBlock-14           [-1, 96, 32, 32]               0\n",
            "           Conv2d-15           [-1, 96, 32, 32]          82,944\n",
            "      BatchNorm2d-16           [-1, 96, 32, 32]             192\n",
            "           Conv2d-17           [-1, 96, 32, 32]          82,944\n",
            "      BatchNorm2d-18           [-1, 96, 32, 32]             192\n",
            "    ResidualBlock-19           [-1, 96, 32, 32]               0\n",
            "           Conv2d-20          [-1, 128, 16, 16]          12,288\n",
            "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
            "           Conv2d-22          [-1, 128, 16, 16]         110,592\n",
            "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
            "           Conv2d-24          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-25          [-1, 128, 16, 16]             256\n",
            "    ResidualBlock-26          [-1, 128, 16, 16]               0\n",
            "           Conv2d-27          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-28          [-1, 128, 16, 16]             256\n",
            "           Conv2d-29          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-30          [-1, 128, 16, 16]             256\n",
            "    ResidualBlock-31          [-1, 128, 16, 16]               0\n",
            "           Conv2d-32            [-1, 240, 8, 8]          30,720\n",
            "      BatchNorm2d-33            [-1, 240, 8, 8]             480\n",
            "           Conv2d-34            [-1, 240, 8, 8]         276,480\n",
            "      BatchNorm2d-35            [-1, 240, 8, 8]             480\n",
            "           Conv2d-36            [-1, 240, 8, 8]         518,400\n",
            "      BatchNorm2d-37            [-1, 240, 8, 8]             480\n",
            "    ResidualBlock-38            [-1, 240, 8, 8]               0\n",
            "           Conv2d-39            [-1, 240, 8, 8]         518,400\n",
            "      BatchNorm2d-40            [-1, 240, 8, 8]             480\n",
            "           Conv2d-41            [-1, 240, 8, 8]         518,400\n",
            "      BatchNorm2d-42            [-1, 240, 8, 8]             480\n",
            "    ResidualBlock-43            [-1, 240, 8, 8]               0\n",
            "           Conv2d-44            [-1, 344, 4, 4]          82,560\n",
            "      BatchNorm2d-45            [-1, 344, 4, 4]             688\n",
            "           Conv2d-46            [-1, 344, 4, 4]         743,040\n",
            "      BatchNorm2d-47            [-1, 344, 4, 4]             688\n",
            "           Conv2d-48            [-1, 344, 4, 4]       1,065,024\n",
            "      BatchNorm2d-49            [-1, 344, 4, 4]             688\n",
            "    ResidualBlock-50            [-1, 344, 4, 4]               0\n",
            "AdaptiveAvgPool2d-51            [-1, 344, 1, 1]               0\n",
            "           Linear-52                   [-1, 10]           3,450\n",
            "================================================================\n",
            "Total params: 4,794,958\n",
            "Trainable params: 4,794,958\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 18.27\n",
            "Params size (MB): 18.29\n",
            "Estimated Total Size (MB): 36.57\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Total Trainable Parameters: 4,794,958 (4.79M)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model Initialization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = OptimizedResNet().to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcdx4NOv_2H9"
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 Data\n",
        "batch_size = 128\n",
        "train_loader, test_loader = get_data(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8AL_uTJ_3qi"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/dl project1/best_resnet.pth'\n",
        "# model_path =''\n",
        "if model_path:\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        print(\"Loaded model checkpoint from\", model_path)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model checkpoint: {e}. Training from scratch.\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Training from scratch.\")\n",
        "\n",
        "num_epochs = 100\n",
        "patience = 3  # Number of epochs to wait before stopping if loss increases and accuracy decreases\n",
        "best_accuracy = 0\n",
        "lowest_loss = float('inf')\n",
        "no_improve_count = 0  # Counter for early stopping\n",
        "\n",
        "best_model_path = '/content/drive/MyDrive/dl project1/best_resnet.pth'  # Save best model separately\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = 100. * correct / total\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%, LR: {scheduler.get_last_lr()}')\n",
        "\n",
        "    def count_parameters(model):\n",
        "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    num_params = count_parameters(model)\n",
        "    print(f\"Total Trainable Parameters: {num_params/1e6:.2f}M\")\n",
        "\n",
        "    # Save the best model\n",
        "    if epoch_accuracy > best_accuracy or epoch_loss < lowest_loss:\n",
        "        best_accuracy = epoch_accuracy\n",
        "        lowest_loss = epoch_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Best model saved at Epoch {epoch+1} with Accuracy: {best_accuracy:.2f}% and Loss: {lowest_loss:.4f}\")\n",
        "\n",
        "    # Early Stopping Logic\n",
        "    if epoch_accuracy < best_accuracy and epoch_loss > lowest_loss:\n",
        "        no_improve_count += 1\n",
        "        print(f\"Warning: Accuracy decreased & Loss increased for {no_improve_count} epochs.\")\n",
        "    else:\n",
        "        no_improve_count = 0  # Reset counter if model improves\n",
        "\n",
        "    if no_improve_count >= patience:\n",
        "        print(\"Early stopping triggered. Stopping training.\")\n",
        "        break  # Exit training loop\n",
        "\n",
        "print(\"Training complete. Loading the best model before evaluation.\")\n",
        "model.load_state_dict(torch.load(best_model_path))  # Load the best model for testing\n",
        "\n",
        "# Testing Loop\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "print(f'Test Accuracy: {100.*correct/total:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFfg3ZmY_63j"
      },
      "outputs": [],
      "source": [
        "# Create a dataset from the images array\n",
        "class CIFAR_Dataset_PKL_Load(Dataset):\n",
        "    def __init__(self, images, transform=None):\n",
        "        self.images = images\n",
        "        if transform is None:\n",
        "            # Default transform: convert numpy array to PIL Image, then to tensor, then normalize\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.images[idx]\n",
        "        if isinstance(img, np.ndarray):\n",
        "            img = img.astype('uint8')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBbz9hKz_77f",
        "outputId": "8ddceef2-0bc7-4f02-ea3e-35cc24f7b66f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded test batch with 10000 images\n",
            "Error loading model checkpoint: [Errno 2] No such file or directory: 'trial'. Using the existing model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-f56d4da0d37d>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to submission_trial2025-03-14 03:45:48.226197.csv\n"
          ]
        }
      ],
      "source": [
        "# model_path = '/content/drive/MyDrive/dl project1/best_resnet.pth'\n",
        "model_path=''\n",
        "# Load the test batch (update the file path if necessary)\n",
        "cifar10_batch = load_cifar_batch('/content/drive/MyDrive/dl project1/data/cifar_test_nolabel.pkl')\n",
        "images = cifar10_batch[b'data']\n",
        "print(f\"Loaded test batch with {images.shape[0]} images\")\n",
        "\n",
        "final_dataset = CIFAR_Dataset_PKL_Load(images)\n",
        "final_loader = DataLoader(final_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def generate_predictions_csv(model_path, model, test_loader, device):\n",
        "    if model_path:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            print(\"Loaded model checkpoint from\", model_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model checkpoint: {e}. Using the existing model.\")\n",
        "    else:\n",
        "        print(\"No checkpoint found. Running inference with the existing model.\")\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in final_loader:\n",
        "            batch = batch.to(device)\n",
        "            outputs = model(batch)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            predictions.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "    n1 = datetime.now()\n",
        "    filename = 'submission_' + model_name + str(n1) + '.csv'\n",
        "    df = pd.DataFrame({\"ID\": range(len(predictions)), \"Labels\": predictions})\n",
        "    filepath = os.path.join('/content/drive/MyDrive/dl project1', filename)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(\"Predictions saved to \"+ filename)\n",
        "\n",
        "model_name = 'trial'\n",
        "generate_predictions_csv(model_name, model, final_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgV6jPb6F8eg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
